{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E78.0 Disease Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and combining all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/jovyan/amedes/credentials/GZFiles/amedes_challenge/data/interim/data_interim_essed.csv') data_extract_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Berlin = data[data.ZENTRUM_ID == 'BER01']\n",
    "Frankfurt = data[data.ZENTRUM_ID == 'FRA01']\n",
    "Hamburg = data[data.ZENTRUM_ID == 'HAM08']\n",
    "Stuttgart = data[data.ZENTRUM_ID == 'STR01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of lab results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def result_inference(x):\n",
    "        result=x['Result']\n",
    "        if r'(' in result:\n",
    "            index=result.index(r'(')\n",
    "            interval=result[index:]\n",
    "            result=result[:index]\n",
    "        if '--' in result:\n",
    "            return 'Very low'\n",
    "        elif '-' in result:\n",
    "            return 'Low'\n",
    "        elif '++' in result:\n",
    "            return 'Very high'\n",
    "        elif '+' in result:\n",
    "            return 'High'\n",
    "        elif 'negativ' in result.lower():\n",
    "            return 'Negative'\n",
    "        elif 'positiv' in result.lower():\n",
    "            return 'Positive'\n",
    "        return 'Normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_processing(data):\n",
    "    Y=data[data['TYP']=='Y']\n",
    "    # Splitting rows with tests seperated by ';' eg HKT=43.0 %; MCV=87.4 fl; MCH=27.6 pg\n",
    "    Y=Y.set_index(['PATIENT_HASH', 'ZENTRUM_ID', 'PATIENT_ID', 'PAT_GEBDATUM',\n",
    "           'PAT_GESCHLECHT', 'DATUM', 'TYP', 'TYP_EXT',  'ICD10',\n",
    "           'SICHERHEIT'])\n",
    "    Y=Y['TEXT'].str.split(';').explode().reset_index()\n",
    "    Y['TEXT']=Y['TEXT'].str.replace(r'^\\s+',r'')\n",
    "    \n",
    "     # Format eg. Eosinophils 10+ % (2 - 4)\n",
    "    Y_1=Y[Y.TEXT.str.contains(r'\\([.0-9\\s]+-[.0-9\\s]+\\)')].copy()\n",
    "    Y_1[['Lab_test','Result']]=Y_1['TEXT'].str.split('\\s',expand=True,n=1)\n",
    "    Y_1.dropna(inplace=True)\n",
    "    Y_1['Inference']=Y_1['Result'].apply(result_inference,axis=1)\n",
    "    \n",
    "     # Format LEUKO=4.5\n",
    "    Y_2=Y[Y.TEXT.str.contains(r'[a-zA-Z0-9\\s]+=[a-zA-Z0-9\\s]+')]\n",
    "    # Seperating Lab tests to their test name and results by searching for a '='\n",
    "    Y_2[['Lab_test','Result']]=Y_2['TEXT'].str.split('=',expand=True,n=1)\n",
    "    Y_2['Inference']=Y_2.apply(result_inference,axis=1)\n",
    "    \n",
    "    # Merge the data frames of the two formats\n",
    "    Y_new=pd.concat([Y_1,Y_2],axis=0)\n",
    "        \n",
    "    return Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_new=lab_processing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def page(data):\n",
    "    \n",
    "    def DATUM_preprocessing(data):\n",
    "        data['entry_year'] = data.DATUM.apply(lambda x: str(x)[6:])\n",
    "        \n",
    "        # dealing with typos in the Berlin dataset\n",
    "        if data.iloc[0,1] == 'BER01':\n",
    "            data['entry_year'] = data['entry_year'].replace('50','05').replace('90','09')\n",
    "            \n",
    "        def extract_year(year):\n",
    "            if year<'21':\n",
    "                    return '20'+year\n",
    "            else:\n",
    "                return '19'+year\n",
    "\n",
    "        data['entry_year'] = data['entry_year'].apply(lambda x: extract_year(x))\n",
    "        _ = data['DATUM'].apply(lambda x: x[:6]) + data['entry_year']\n",
    "        data['entry_date'] = pd.to_datetime(_, format='%d.%m.%Y')\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def PAT_GEBDATUM_preprocessing(data):\n",
    "  \n",
    "        data['birth_year'] = data.PAT_GEBDATUM.apply(lambda x: str(x)[6:])\n",
    "        data['birth_entry'] = data['entry_year'].apply(lambda x: str(x)[2:]).astype(int) - data['birth_year'].astype(int)\n",
    "        \n",
    "        def extract_year(row):\n",
    "            if row.birth_year > '20':\n",
    "                return '19' + row.birth_year\n",
    "            else:\n",
    "                if row.entry_year < '2000':\n",
    "                    return '19' + row.birth_year\n",
    "                else :\n",
    "                    if row.birth_entry < 0: \n",
    "                        return '19' + row.birth_year\n",
    "                    else:\n",
    "                        return '20' + row.birth_year\n",
    "             \n",
    "                                      \n",
    "        data.loc[:, 'birth_year'] = data.apply(extract_year, axis = 1)  \n",
    "        \n",
    "        _ = data['PAT_GEBDATUM'].apply(lambda x: x[:6]) + data['birth_year']\n",
    "        data['birth_date'] = pd.to_datetime(_, format='%d.%m.%Y')\n",
    "        \n",
    "        data = data.drop(['birth_year', 'entry_year', 'birth_entry'], axis=1)\n",
    "        \n",
    "        return data\n",
    "    def age( data):\n",
    "\n",
    "        _ = data['entry_date'] - data['birth_date']\n",
    "        data['age'] = (_ / np.timedelta64(1, 'Y')).round(1)\n",
    "        \n",
    "        return data   \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=page(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and encoding test results most commonly found in patients suffering from Famili채re Hypercholesterin채mie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to replace values with numbers(Can be used as ordinal values)\n",
    "def Inference(row):\n",
    "    if row['Inference'] =='Low':\n",
    "        return 1\n",
    "    if row['Inference'] =='High':\n",
    "        return 1\n",
    "    if row['Inference'] =='Very high':\n",
    "        return 2\n",
    "    if row['Inference'] =='Very low':\n",
    "        return 2\n",
    "    if row['Inference'] =='Negative':\n",
    "        return 2\n",
    "    if row['Inference'] =='Normal':\n",
    "        return 0\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv(findata,Y_new):\n",
    "    #drop the text columns\n",
    "    findata=findata.dropna(subset=['TEXT'])\n",
    "    #make the final column \"A\"-absent in the presence of \"usschl\" in any row for any disease\n",
    "    findata.loc[findata['TEXT'].str.contains('usschl'), ['SICHERHEIT']] = 'A'\n",
    "    #make lab results columns instead of rows\n",
    "    nolab=findata[findata != 'Y']\n",
    "    #y_3 contains the desired rows\n",
    "    Y_3=Y_new[[\"PATIENT_HASH\",'DATUM','Lab_test','Result','Inference']]\n",
    "    #group patients text to get the rows combined(later used for filtering)\n",
    "    Y_text=nolab.groupby(['PATIENT_HASH'])['TEXT'].apply(','.join).reset_index()\n",
    "    #merge the text to the lab results to filter based on diseases\n",
    "    text_lab=Y_3.merge(Y_text,on='PATIENT_HASH',how='left')\n",
    "    #filter the result finally\n",
    "    text_lab=text_lab[(text_lab['Inference']!='Normal')&(text_lab.TEXT.str.contains('Famili채re Hypercholesterin채mie'))]\n",
    "    #selected only 20 top lab tests\n",
    "    text_interest=pd.DataFrame(text_lab.Lab_test.value_counts())\n",
    "    list=text_interest.reset_index().head(20).iloc[:, 0]\n",
    "    pivot=Y_3[(Y_3.Lab_test.isin(list))]\n",
    "    pivot['label'] = pivot.apply (lambda row: Inference(row), axis=1)\n",
    "    merge=pd.pivot_table(pivot, values = 'label', index=['PATIENT_HASH','DATUM'], columns = 'Lab_test').reset_index()\n",
    "    final_with_lab=nolab.merge(merge,on=['PATIENT_HASH','DATUM'],how='left')\n",
    "    #replace ICD10 na with \"none\"\n",
    "    final_with_lab.ICD10.fillna('none',inplace=True)\n",
    "    #drop values where sich is nan\n",
    "    final_with_lab.dropna(subset=['SICHERHEIT'],inplace=True)\n",
    "    final_with_lab=final_with_lab.fillna(0)\n",
    "    return final_with_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab=piv(data,Y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the status 1 for presence & status 0 for absence of disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Status(final_with_lab):\n",
    "    l = ['amili채re Hyperchol', 'amili채re hyperchol', 'amili채ren Hyperchol', 'amili채ren hyperchol']  \n",
    "    regstr = '|'.join(l)\n",
    "    final_with_lab.loc[(final_with_lab['TEXT'].str.contains(regstr)),'ICD10']='E78.01'\n",
    "    final_with_lab.loc[(final_with_lab['SICHERHEIT'].isin(['G','Z'])) & (final_with_lab['ICD10'].isin(['E78.01']))& (final_with_lab['TYP'].isin(['*'])), 'Status'] = 1\n",
    "    final_with_lab.Status.fillna(0,inplace=True)\n",
    "    return final_with_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab=Status(final_with_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding clusters to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the cluster files\n",
    "clusters=pd.read_csv(\"/home/jovyan/amedes/credentials/GZFiles/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clus(clusters,final_with_lab):\n",
    "     #clusters.rename({'NEW_COLUMN':'clusters'},axis=1,inplace=True)\n",
    "    clusters=clusters[['PATIENT_HASH','clusters']]\n",
    "    final_with_lab=final_with_lab.merge(clusters,on='PATIENT_HASH',how='left')\n",
    "    return final_with_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab=clus(clusters,final_with_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(final_with_lab):\n",
    "    model=final_with_lab.drop(columns=['TYP','SICHERHEIT'])\n",
    "    #mean encode ICD10 as feature\n",
    "    mean_encode=model.groupby('ICD10').size()/len(model)\n",
    "    model.loc[:,'ICD_mean_encode']=model['ICD10'].map(mean_encode)\n",
    "    model.drop(columns=['ICD10'],inplace=True)\n",
    "    \n",
    "    #grouping columns from the model to obtain individual id's\n",
    "    #model1\n",
    "    m1=model[['PATIENT_HASH','is_E75.22', 'is_E78.01',\n",
    "       'is_E78.3', 'is_E71.3', 'is_disease', 'CHOL', 'CRP', 'ERY', 'FT4', 'GGT', 'GLU', 'GPT', 'HB', 'HDL',\n",
    "       'HKT', 'HS', 'LDL', 'MCH', 'MCHC', 'MCV', 'TRI', 'TRIG', 'TSH','TPO',\n",
    "       'VITD25','ICD_mean_encode','Status']]\n",
    "    m1=m1.groupby(['PATIENT_HASH']).sum()\n",
    "    #model 2\n",
    "    m2=model[['PATIENT_HASH','clusters','PAT_GESCHLECHT','ZENTRUM_ID','PATIENT_ID']]\n",
    "    m2=m2.groupby(['PATIENT_HASH']).last()\n",
    "    #merge the two\n",
    "    model1=m1.merge(m2,on=['PATIENT_HASH'],how='left')\n",
    "    #fill the clusters\n",
    "    model1.clusters=model1.clusters.fillna(100)\n",
    "    #make non zero or one status one:\n",
    "    model1.loc[model1['Status']==2, 'Status'] = 1\n",
    "    #subset the columns\n",
    "    model1=model1[['CHOL', 'CRP', 'ERY', 'FT4', 'GGT', 'GLU', 'GPT', 'HB', 'HDL',\n",
    "       'HKT', 'HS', 'LDL', 'MCH', 'MCHC', 'MCV', 'TRI', 'TRIG', 'TSH','TPO',\n",
    "       'VITD25','ICD_mean_encode','Status','PAT_GESCHLECHT','ZENTRUM_ID',\n",
    "        'clusters','is_E71.3','is_E75.22', 'is_E78.01' ,'is_E78.3','is_disease']]\n",
    "    #dummy encode zentrum and pad\n",
    "    model1 = pd.get_dummies(model1, columns=['ZENTRUM_ID'], drop_first=True)\n",
    "    model1= pd.get_dummies(model1, columns=['PAT_GESCHLECHT'], drop_first=True)\n",
    "    #train&test split by sampling\n",
    "    df1 = model1[model1.Status == 0].sample(1000)\n",
    "    df2 = model1[~model1.index.isin(df1.index)]\n",
    "    X=df2.loc[:, df2.columns != 'Status']\n",
    "    y=df2.loc[:, df2.columns == 'Status']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8675309)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=encoding(final_with_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exps(X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners\n",
    ":param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    models = [\n",
    "              ('LogReg', LogisticRegression()), \n",
    "              ('RF', RandomForestClassifier()),\n",
    "              ('KNN', KNeighborsClassifier()),\n",
    "              ('SVM', SVC()), \n",
    "              ('GNB', GaussianNB()),\n",
    "              ('XGB', XGBClassifier())\n",
    "            ]\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "    target_names = ['no_e78', 'yes_e78']\n",
    "    for name, model in models:\n",
    "            kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "            cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "            clf = model.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            print(name)\n",
    "            print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "            print(confusion_matrix(y_test, y_pred,labels=[0,1]))\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    this_df = pd.DataFrame(cv_results)\n",
    "    this_df['model'] = name\n",
    "    dfs.append(this_df)\n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=run_exps(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_model(X_train, X_test, y_train, y_test,final_with_lab):\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    #import shap\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "    cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    clf = rf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    print(confusion_matrix(y_test, y_pred,labels=[0,1]))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    # view the feature scores\n",
    "    feature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "    # Creating a seaborn bar plot\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(30, 24))\n",
    "    ax = sns.barplot(x=feature_scores, y=feature_scores.index, data=final_with_lab)\n",
    "    ax.set_title(\"Visualize feature scores of the features\")\n",
    "    ax.set_yticklabels(feature_scores.index)\n",
    "    ax.set_xlabel(\"Feature importance score\")\n",
    "    ax.set_ylabel(\"Features\")\n",
    "    plt.show()\n",
    "    score=accuracy_score(y_test,y_pred)\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=best_model(X_train, X_test, y_train, y_test,final_with_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model(X_train, X_test, y_train, y_test,final_with_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting LAB RESULTS COLUMNS INSTEAD OF ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nolab=findata[findata != 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_3 contains the desired rows\n",
    "Y_3=Y_new[[\"PATIENT_HASH\",'DATUM','Lab_test','Result','Inference']]\n",
    "#group patients text to get the rows combined(later used for filtering)\n",
    "Y_text=nolab.groupby(['PATIENT_HASH'])['TEXT'].apply(','.join).reset_index()\n",
    "#merge the text to the lab results to filter based on diseases\n",
    "text_lab=Y_3.merge(Y_text,on='PATIENT_HASH',how='left')\n",
    "#filter the result finally\n",
    "text_lab=text_lab[(text_lab['Inference']!='Normal')&(text_lab.TEXT.str.contains('Famili채re Hypercholesterin채mie'))]\n",
    "#selected only 20 top lab tests\n",
    "text_interest=pd.DataFrame(text_lab.Lab_test.value_counts())\n",
    "list=text_interest.reset_index().head(20).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot=Y_3[(Y_3.Lab_test.isin(list))]\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to replace values with numbers(Can be used as ordinal values)\n",
    "def Inference(row):\n",
    "    if row['Inference'] =='Low':\n",
    "        return 1\n",
    "    if row['Inference'] =='High':\n",
    "        return 1\n",
    "    if row['Inference'] =='Very high':\n",
    "        return 2\n",
    "    if row['Inference'] =='Very low':\n",
    "        return 2\n",
    "    if row['Inference'] =='Negative':\n",
    "        return 2\n",
    "    if row['Inference'] =='Normal':\n",
    "        return 0\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot['label'] = pivot.apply (lambda row: Inference(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge=pd.pivot_table(pivot, values = 'label', index=['PATIENT_HASH','DATUM'], columns = 'Lab_test').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab=nolab.merge(merge,on=['PATIENT_HASH','DATUM'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace ICD10 na with \"none\"\n",
    "final_with_lab.ICD10.fillna('none',inplace=True)\n",
    "#drop values where sich is nan\n",
    "final_with_lab.dropna(subset=['SICHERHEIT'],inplace=True)\n",
    "final_with_lab=final_with_lab.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we add the cluster later on &now we can just move to status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['amili채re Hyperchol', 'amili채re hyperchol', 'amili채ren Hyperchol', 'amili채ren hyperchol']  \n",
    "regstr = '|'.join(l)\n",
    "final_with_lab.loc[(final_with_lab['TEXT'].str.contains(regstr)),'ICD10']='E78.01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab.loc[(final_with_lab['SICHERHEIT'].isin(['G','Z'])) & (nolab['ICD10'].isin(['E78.01']))& (nolab['TYP'].isin(['*'])), 'Status'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab.Status.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab.loc[(final_with_lab['TEXT'].str.contains(regstr))].TYP.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab[(final_with_lab['TYP'].isin(['*'])) & (final_with_lab.TEXT.str.contains('Famili채re Hypercholesterin채mie'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding clusterwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the cluster files\n",
    "clusters=pd.read_csv(\"/home/jovyan/amedes/credentials/GZFiles/NEW-Copy1.csv\")\n",
    "clusters.rename({'NEW_COLUMN':'clusters'},axis=1,inplace=True)\n",
    "clusters=clusters[['PATIENT_HASH','clusters']]\n",
    "clusters.clusters.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab=final_with_lab.merge(clusters,on='PATIENT_HASH',how='left')\n",
    "final_with_lab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_with_lab.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model=final_with_lab.drop(columns=['TYP','SICHERHEIT'])\n",
    "    #mean encode ICD10 as feature\n",
    "    mean_encode=model.groupby('ICD10').size()/len(model)\n",
    "    model.loc[:,'ICD_mean_encode']=model['ICD10'].map(mean_encode)\n",
    "    model.drop(columns=['ICD10'],inplace=True)\n",
    "    model.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(final_with_lab):\n",
    "    model=final_with_lab.drop(columns=['TYP','SICHERHEIT'])\n",
    "    #mean encode ICD10 as feature\n",
    "    mean_encode=model.groupby('ICD10').size()/len(model)\n",
    "    model.loc[:,'ICD_mean_encode']=model['ICD10'].map(mean_encode)\n",
    "    model.drop(columns=['ICD10'],inplace=True)\n",
    "    \n",
    "    #grouping columns from the model to obtain individual id's\n",
    "    #model1\n",
    "    m1=model[['PATIENT_HASH','is_E75.22', 'is_E78.01',\n",
    "       'is_E78.3', 'is_E71.3', 'is_disease', 'CHOL', 'CRP', 'ERY', 'FT4', 'GGT', 'GLU', 'GPT', 'HB', 'HDL',\n",
    "       'HKT', 'HS', 'LDL', 'MCH', 'MCHC', 'MCV', 'TRI', 'TRIG', 'TSH','TPO',\n",
    "       'VITD25','ICD_mean_encode','Status']]\n",
    "    m1=m1.groupby(['PATIENT_HASH']).sum()\n",
    "    #model 2\n",
    "    m2=model[['PATIENT_HASH','clusters','PAT_GESCHLECHT','ZENTRUM_ID','PATIENT_ID']]\n",
    "    m2=m2.groupby(['PATIENT_HASH']).last()\n",
    "    #merge the two\n",
    "    model1=m1.merge(m2,on=['PATIENT_HASH'],how='left')\n",
    "    #fill the clusters\n",
    "    model1.clusters=model1.clusters.fillna(100)\n",
    "    #make non zero or one status one:\n",
    "    model1.loc[model1['Status']==2, 'Status'] = 1\n",
    "    #subset the columns\n",
    "    model1=model1[['CHOL', 'CRP', 'ERY', 'FT4', 'GGT', 'GLU', 'GPT', 'HB', 'HDL',\n",
    "       'HKT', 'HS', 'LDL', 'MCH', 'MCHC', 'MCV', 'TRI', 'TRIG', 'TSH','TPO',\n",
    "       'VITD25','ICD_mean_encode','Status','PAT_GESCHLECHT','ZENTRUM_ID',\n",
    "        'clusters','is_E71.3','is_E75.22', 'is_E78.01' ,'is_E78.3','is_disease']]\n",
    "    #dummy encode zentrum and pad\n",
    "    model1 = pd.get_dummies(model1, columns=['ZENTRUM_ID'], drop_first=True)\n",
    "    model1= pd.get_dummies(model1, columns=['PAT_GESCHLECHT'], drop_first=True)\n",
    "    #train&test split by sampling\n",
    "    df1 = model1[model1.Status == 0].sample(1000)\n",
    "    df2 = model1[~model1.index.isin(df1.index)]\n",
    "    X=df2.loc[:, df2.columns != 'Status']\n",
    "    y=df2.loc[:, df2.columns == 'Status']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8675309)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=encoding(final_with_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop the not needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=final_with_lab.drop(columns=['TYP','SICHERHEIT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEAN ENCODE ICD CODE AS FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encode=model.groupby('ICD10').size()/len(model)\n",
    "model.loc[:,'ICD_mean_encode']=model['ICD10'].map(mean_encode)\n",
    "model.drop(columns=['ICD10'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=model[['PATIENT_HASH','is_E75.22', 'is_E78.0',\n",
    "       'is_E78.3', 'is_E71.3', 'is_disease', 'CHOL', 'CRP', 'ERY', 'FT4', 'GGT', 'GLU', 'GPT', 'HB', 'HDL',\n",
    "       'HKT', 'HS', 'LDL', 'MCH', 'MCHC', 'MCV', 'TRI', 'TRIG', 'TSH',\n",
    "       'VITD25','ICD_mean_encode','Status']]\n",
    "m1=m1.groupby(['PATIENT_HASH']).sum()\n",
    "m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=model[['PATIENT_HASH','clusters','PAT_GESCHLECHT','ZENTRUM_ID','PATIENT_ID']]\n",
    "#df.groupby(['id','product']).last()\n",
    "m2=m2.groupby(['PATIENT_HASH']).last()\n",
    "m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=m1.merge(m2,on=['PATIENT_HASH'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.clusters=model1.clusters.fillna(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.loc[model1['Status']==2, 'Status'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=model1[['CHOL',\n",
    " 'CRP',\n",
    " 'ERY',\n",
    " 'FT4',\n",
    " 'GGT',\n",
    " 'GLU',\n",
    " 'GPT',\n",
    " 'HB',\n",
    " 'HDL',\n",
    " 'HKT',\n",
    " 'HS',\n",
    " 'ICD_mean_encode',\n",
    " 'LDL',\n",
    " 'MCH',\n",
    " 'MCHC',\n",
    " 'MCV',\n",
    " 'PATIENT_ID',\n",
    " 'PAT_GESCHLECHT',\n",
    " 'Status',\n",
    " 'TRI',\n",
    " 'TRIG',\n",
    " 'TSH',\n",
    " 'VITD25',\n",
    " 'ZENTRUM_ID',\n",
    " 'clusters',\n",
    " 'is_E71.3',\n",
    " 'is_E75.22',\n",
    " 'is_E78.0',\n",
    " 'is_E78.3',\n",
    " 'is_disease']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy encode zentrum &pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pd.get_dummies(model1, columns=['ZENTRUM_ID'], drop_first=True)\n",
    "model1= pd.get_dummies(model1, columns=['PAT_GESCHLECHT'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = model1[model1.Status == 0].sample(1000)\n",
    "df2 = model1[~model1.index.isin(df1.index)]\n",
    "X=df2.loc[:, df2.columns != 'Status']\n",
    "y=df2.loc[:, df2.columns == 'Status']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8675309)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners\n",
    ":param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "models = [\n",
    "          ('LogReg', LogisticRegression()), \n",
    "          ('RF', RandomForestClassifier()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('SVM', SVC()), \n",
    "          ('GNB', GaussianNB()),\n",
    "          ('XGB', XGBClassifier())\n",
    "        ]\n",
    "results = []\n",
    "names = []\n",
    "scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "target_names = ['no_e78', 'yes_e78']\n",
    "for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        print(confusion_matrix(y_test, y_pred,labels=[0,1]))\n",
    "results.append(cv_results)\n",
    "names.append(name)\n",
    "this_df = pd.DataFrame(cv_results)\n",
    "this_df['model'] = name\n",
    "dfs.append(this_df)\n",
    "final = pd.concat(dfs, ignore_index=True)\n",
    "return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
