{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaucher Disease Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from collections import Counter    \n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/jovyan/amedes_challenge/data/interim/data_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Berlin = data[data.ZENTRUM_ID == 'BER01']\n",
    "Frankfurt = data[data.ZENTRUM_ID == 'FRA01']\n",
    "Hamburg = data[data.ZENTRUM_ID == 'HAM08']\n",
    "Stuttgart = data[data.ZENTRUM_ID == 'STR01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of lab results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_processing(center, ICD):\n",
    "\n",
    "    patients = center[(center.SICHERHEIT == 'G') & (center.ICD10 == ICD)].PATIENT_HASH.unique()\n",
    "    center = center[center.PATIENT_HASH.isin(patients)]\n",
    "\n",
    "    Y=center[center['TYP']=='Y'].dropna(subset=[\"TEXT\"])\n",
    "    Y=Y.drop_duplicates()\n",
    "    \n",
    "    def result_inference(x):\n",
    "        result=x['Result']\n",
    "        if r'(' in result:\n",
    "            index=result.index(r'(')\n",
    "            interval=result[index:]\n",
    "            result=result[:index]\n",
    "        if '--' in result:\n",
    "            return 'Very low'\n",
    "        elif '-' in result:\n",
    "            return 'Low'\n",
    "        elif '++' in result:\n",
    "            return 'Very high'\n",
    "        elif '+' in result:\n",
    "            return 'High'\n",
    "        elif 'negativ' in result.lower():\n",
    "            return 'Negative'\n",
    "        elif 'positiv' in result.lower():\n",
    "            return 'Positive'\n",
    "        return 'Normal'\n",
    "    \n",
    "    # Splitting rows with tests seperated by ';' eg HKT=43.0 %; MCV=87.4 fl; MCH=27.6 pg\n",
    "    Y['text'] = Y['TEXT']\n",
    "    Y=Y.set_index(['PATIENT_HASH', 'ZENTRUM_ID', 'PATIENT_ID', 'PAT_GEBDATUM',\n",
    "           'PAT_GESCHLECHT', 'DATUM', 'TYP', 'TYP_EXT', 'text','ICD10',\n",
    "           'SICHERHEIT'])\n",
    "    Y=Y['TEXT'].str.split(';').explode().reset_index()\n",
    "    Y['TEXT']=Y['TEXT'].str.replace(r'^\\s+',r'')\n",
    "    \n",
    "    \n",
    "    # Format LEUKO=4.5\n",
    "    Y_2=Y[Y.TEXT.dropna().str.contains(r'[a-zA-Z0-9\\s]+=[a-zA-Z0-9\\s]+')]\n",
    "    Y_2[['Lab_test','Result']]=Y_2['TEXT'].str.split('=',expand=True,n=1)\n",
    "    Y_2['Inference']=Y_2.apply(result_inference,axis=1)\n",
    "    Y_2['Inference'].value_counts() \n",
    "    Y_new=Y_2\n",
    "        \n",
    "    # Find tests that appear in the dataframe atleast 10 times\n",
    "    counts=Y_new['Lab_test'].value_counts().reset_index()\n",
    "    tests=list(set(counts[counts['Lab_test']>1]['index']))\n",
    "    Y_new=Y_new[Y_new['Lab_test'].isin(tests)]\n",
    "\n",
    "    # Group by Patient and Date and create two dictionaries of Lab_test:Inference, Lab_test:Result\n",
    "    columns=['PATIENT_HASH', 'ZENTRUM_ID', 'PATIENT_ID', 'PAT_GEBDATUM',\n",
    "           'PAT_GESCHLECHT', 'DATUM','TYP','text']\n",
    "    Y_new=Y_new.set_index('Lab_test').groupby(columns)[['Inference','Result']].apply(lambda x: x.to_dict()).reset_index(name='Lab_results')\n",
    "    Y_new['Inference_dict']=Y_new['Lab_results'].apply(lambda x: x['Inference'])\n",
    "    Y_new['Result_dict']=Y_new['Lab_results'].apply(lambda x: x['Result'])\n",
    "    Y_new=Y_new.drop('Lab_results',axis=1)\n",
    "    \n",
    "    # Convert Inference dictionary to multiple columns, one for each test\n",
    "    results_df=Y_new[\"Inference_dict\"].apply(pd.Series)\n",
    "    Y_new=pd.concat([Y_new,results_df],axis=1)\n",
    "    \n",
    "    Y_new = Y_new.drop(columns=['PATIENT_ID','PAT_GEBDATUM','PAT_GESCHLECHT','Result_dict','Inference_dict']).rename(columns={\"text\":\"TEXT\"})\n",
    "        \n",
    "    return Y_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaucher disease "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICD = 'E75.22'\n",
    "Berlin = lab_processing(Berlin,ICD)\n",
    "print(\"Berlin done\")\n",
    "Frankfurt = lab_processing(Frankfurt,ICD)\n",
    "print(\"Frankfurt done\")\n",
    "Hamburg = lab_processing(Hamburg,ICD)\n",
    "print('Hamburg done')\n",
    "Stuttgart = lab_processing(Stuttgart,ICD)\n",
    "print(\"Stuttgart done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [Berlin, Frankfurt, Hamburg, Stuttgart]\n",
    "Y_new = pd.concat(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = data[(data.SICHERHEIT == 'G') & (data.ICD10 == ICD)].PATIENT_HASH.unique()\n",
    "diagnosed = data[data.PATIENT_HASH.isin(patients)]\n",
    "diagnosed = diagnosed.merge(Y_new[Y_new.PATIENT_HASH.isin(patients)], \n",
    "                                                how='left', on=['PATIENT_HASH','ZENTRUM_ID','DATUM','TYP','TEXT'])\n",
    "diagnosed = diagnosed.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  diagnosed[diagnosed.TYP == \"Y\"].replace(\"Normal\",\"\").fillna(\"\") \\\n",
    "    .groupby('PATIENT_HASH')[np.array(diagnosed.columns[19:])].sum() \\\n",
    "    .replace(0,np.nan).dropna(how='all', axis=0).reset_index()\n",
    "\n",
    "def value(x):\n",
    "    if x==\"\":\n",
    "        return x\n",
    "    else:\n",
    "        return ['Low','High','Very low','Very high','Normal','Positive','Negative'][\n",
    "            np.argmax([x.count('Low'),x.count('High'),x.count('Very low'),x.count('Very high'),x.count('Normal'),\n",
    "                      x.count('Positive'),x.count('Negative')])]\n",
    "\n",
    "y = pd.concat([x.PATIENT_HASH,x.drop('PATIENT_HASH', axis=1).applymap(value)], axis=1)\n",
    "y = pd.DataFrame(data=y.drop('PATIENT_HASH', axis=1).sum(axis=0), columns=['occurences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['High'] = y['occurences'].apply(lambda x: str(x).count('High'))\n",
    "y['Low'] = y['occurences'].apply(lambda x: str(x).count('Low'))\n",
    "y['Positive'] = y['occurences'].apply(lambda x: str(x).count('Positive'))\n",
    "y['Negative'] = y['occurences'].apply(lambda x: str(x).count('Negative'))\n",
    "y['#'] = y[['High','Low','Positive','Negative']].sum(axis=1)\n",
    "y['%'] = y[['High','Low','Positive','Negative']].max(axis=1)/y[\"#\"]\n",
    "y['Dominant'] = y[['High','Low','Positive','Negative']].apply(lambda x: ['High','Low','Positive','Negative'][x.argmax()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[(y['#']>5) & (y[\"%\"]>0.6)].sort_values(by=\"#\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tests = y[(y['#']>5) & (y[\"%\"]>0.6)]['Dominant']\n",
    "relevant_tests.columns = ['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(relevant_tests.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_test(row):\n",
    "    tests = np.array(relevant_tests.index)\n",
    "    count = 0\n",
    "    for test in tests:\n",
    "        if row[test] == relevant_tests.loc[test]:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "diagnosed.loc[:,\"relevant_tests\"] = diagnosed.apply(relevant_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosed.groupby('PATIENT_HASH')['relevant_tests'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-morbidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing ICD\n",
    "def ICD(row): \n",
    "    if (row['TYP'] in ['*','D']) & (pd.isnull(row['ICD10'])):\n",
    "        if ('Hyperton' in row.TEXT) & ('art' in row.TEXT):\n",
    "            return 'I10.90'\n",
    "        if 'Hepatitis C' in row.TEXT:\n",
    "            return 'B18.2'\n",
    "        if 'Hypothyreose nach medizinischen Maßnahmen' in row.TEXT:\n",
    "            return 'E89.0'\n",
    "        if 'Sterilität beim Mann' in row.TEXT:\n",
    "            return 'N46'\n",
    "        if '3-Gefäß-KHK' in row.TEXT:\n",
    "            return 'I25.13'\n",
    "        if ('Vit' in row.TEXT) & ('D' in row.TEXT) & ('Mangel' in row.TEXT):\n",
    "            return 'E55.9'\n",
    "        if 'Anämie' in row.TEXT:\n",
    "            return 'D64.9'\n",
    "        if ('fatigue' in row.TEXT) & ('yndrom' in row.TEXT):\n",
    "            return 'G93.3'\n",
    "        if 'Obstruktive Bronchitis' in row.TEXT:\n",
    "            return 'J44.89'\n",
    "        else:\n",
    "            return row['ICD10']\n",
    "    else:\n",
    "        return row['ICD10']\n",
    "diagnosed['ICD10'] = diagnosed.apply(ICD, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = diagnosed[diagnosed.TYP.apply(lambda x: x in [\"*\",\"D\"])][['PATIENT_HASH','ICD10']].groupby('ICD10')['PATIENT_HASH'].count(\n",
    ").reset_index().sort_values(by='PATIENT_HASH',ascending=False)\n",
    "s[s.PATIENT_HASH>3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- E55.9: Vitamine D defficiency\n",
    "- I10.90 : Hypertension\n",
    "- D69.61 : Thrombocytopenia (abnormally low levels of platelets in the blood)\n",
    "- G93.3 : Fatigue Syndrom\n",
    "- R16.1 : Splenomegaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coMorbidity = ['E55.9','E55.9','I10.90','D69.61','G93.3','R16.1']\n",
    "diagnosed[diagnosed.ICD10.apply(lambda x: x in coMorbidity)].groupby('PATIENT_HASH')['ICD10'].nunique().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms = [\"müde\",\"Fatique\",\"Erschöpfung\", \"fatigue\", \"Fatigue\", #fatigue\n",
    "            \"Knochenschmerzen\",\"Knochenstoffwechselstörung\", # bone pain\n",
    "            \"Milzläsionen\", \"Splenomegalie\",\"splenomegalie\", \"Splenektomie\", \"Hepatosplenomegalie\", # spenomagalie\n",
    "            \"Thrombopenie\",\"Thrombozytopenie\", \"Chololithiasis\",\"Chitotriosidase\",\n",
    "            \"Anämie\",\"Leukopenie\",\"Panzytopenie\",\"Niereninsuffizienz\",\"Nephrolithiasis\"]\n",
    "diagnosed['is_symptom'] = diagnosed.TEXT.apply(lambda x: sum([t*1  in x for t in symptoms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = diagnosed[diagnosed.is_symptom>0].sort_values(by=\"is_symptom\", ascending=0)\n",
    "a.groupby('PATIENT_HASH')['is_symptom'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_results_processing(data):\n",
    "\n",
    "    Y=data[data['TYP']=='Y'].dropna(subset=[\"TEXT\"])\n",
    "    Y=Y.drop_duplicates()\n",
    "    \n",
    "    def result_inference(x):\n",
    "        result=x['Result']\n",
    "        if r'(' in result:\n",
    "            index=result.index(r'(')\n",
    "            interval=result[index:]\n",
    "            result=result[:index]\n",
    "        if '--' in result:\n",
    "            return 'Very low'\n",
    "        elif '-' in result:\n",
    "            return 'Low'\n",
    "        elif '++' in result:\n",
    "            return 'Very high'\n",
    "        elif '+' in result:\n",
    "            return 'High'\n",
    "        elif 'negativ' in result.lower():\n",
    "            return 'Negative'\n",
    "        elif 'positiv' in result.lower():\n",
    "            return 'Positive'\n",
    "        return 'Normal'\n",
    "    \n",
    "    # Splitting rows with tests seperated by ';' eg HKT=43.0 %; MCV=87.4 fl; MCH=27.6 pg\n",
    "    Y['text'] = Y['TEXT']\n",
    "    Y=Y.set_index(['PATIENT_HASH', 'ZENTRUM_ID', 'PATIENT_ID', 'PAT_GEBDATUM',\n",
    "           'PAT_GESCHLECHT', 'DATUM', 'TYP', 'TYP_EXT', 'text','ICD10',\n",
    "           'SICHERHEIT'])\n",
    "    Y=Y['TEXT'].str.split(';').explode().reset_index()\n",
    "    Y['TEXT']=Y['TEXT'].str.replace(r'^\\s+',r'')\n",
    "    \n",
    "    \n",
    "    # Format LEUKO=4.5\n",
    "    Y_2=Y[Y.TEXT.dropna().str.contains(r'[a-zA-Z0-9\\s]+=[a-zA-Z0-9\\s]+')]\n",
    "    Y_2[['Lab_test','Result']]=Y_2['TEXT'].str.split('=',expand=True,n=1)\n",
    "    Y_2['Inference']=Y_2.apply(result_inference,axis=1)\n",
    "    Y_2['Inference'].value_counts() \n",
    "    Y_new=Y_2\n",
    "        \n",
    "    # Find tests that appear in the dataframe atleast 10 times\n",
    "    counts=Y_new['Lab_test'].value_counts().reset_index()\n",
    "    tests=list(set(counts[counts['Lab_test']>1]['index']))\n",
    "    Y_new=Y_new[Y_new['Lab_test'].isin(tests)]\n",
    "\n",
    "    # Group by Patient and Date and create two dictionaries of Lab_test:Inference, Lab_test:Result\n",
    "    columns=['PATIENT_HASH', 'ZENTRUM_ID', 'PATIENT_ID', 'PAT_GEBDATUM',\n",
    "           'PAT_GESCHLECHT', 'DATUM','TYP','text']\n",
    "    Y_new=Y_new.set_index('Lab_test').groupby(columns)[['Inference','Result']].apply(lambda x: x.to_dict()).reset_index(name='Lab_results')\n",
    "    Y_new['Inference_dict']=Y_new['Lab_results'].apply(lambda x: x['Inference'])\n",
    "    Y_new['Result_dict']=Y_new['Lab_results'].apply(lambda x: x['Result'])\n",
    "    Y_new=Y_new.drop('Lab_results',axis=1)\n",
    "    \n",
    "    # Convert Inference dictionary to multiple columns, one for each test\n",
    "    results_df=Y_new[\"Inference_dict\"].apply(pd.Series)\n",
    "    Y_new=pd.concat([Y_new,results_df],axis=1)\n",
    "    \n",
    "    Y_new = Y_new.drop(columns=['PATIENT_ID','PAT_GEBDATUM','PAT_GESCHLECHT','Result_dict','Inference_dict']).rename(columns={\"text\":\"TEXT\"})\n",
    "        \n",
    "    return Y_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(data, k):\n",
    "    patients = data[(data.SICHERHEIT == 'G') & (data.ICD10 == 'E75.22')].PATIENT_HASH.unique()\n",
    "\n",
    "    subset_patients = np.concatenate((np.array(random.choices(data[(~data.PATIENT_HASH.isin(patients)) \n",
    "                                                                   & (data['is_E75.22']==0)].PATIENT_HASH.unique(), k=k)),\n",
    "                                      patients))\n",
    "    subset = data[data.PATIENT_HASH.isin(subset_patients)].reset_index(drop=True)\n",
    "\n",
    "    Y_new = lab_results_processing(subset)\n",
    "\n",
    "    subset = subset.merge(Y_new, how='left', on=['PATIENT_HASH','ZENTRUM_ID','DATUM','TYP','TEXT'])\n",
    "    subset = subset.dropna(axis=1, how='all')\n",
    "    \n",
    "    # relevant tests\n",
    "    x = subset[['PATIENT_HASH','TYP','ERY', 'HB', 'HKT', 'MCH', 'THRO', 'AP', 'GGT', 'FERR', 'NEUT',\n",
    "           'FKAP', 'FKALAQ', 'CRP', 'INSU', 'OSTE', 'VD25', 'TFS', 'ALBUMA',\n",
    "           'A1GLOA', 'A2GLOA', 'DPD']]\n",
    "    x =  x[x.TYP == \"Y\"].fillna(\"\") \\\n",
    "        .groupby('PATIENT_HASH')[np.array(x.columns[2:])].sum() \\\n",
    "        .dropna(how='all', axis=0).reset_index()\n",
    "\n",
    "    def value(x):\n",
    "        if x==\"\":\n",
    "            return \"no_test\"\n",
    "        else:\n",
    "            return ['Low','High','Very low','Very high','Normal'][\n",
    "                np.argmax([x.count('Low'),x.count('High'),x.count('Very low'),x.count('Very high'),x.count('Normal')])]\n",
    "\n",
    "    test = pd.concat([x.PATIENT_HASH,x.drop('PATIENT_HASH', axis=1).applymap(value)], axis=1)\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit_transform(test.drop('PATIENT_HASH',axis=1))\n",
    "    relevantTests = ['ERY', 'HB', 'HKT', 'MCH', 'THRO', 'AP', 'GGT', 'FERR', 'NEUT','FKAP', 'FKALAQ', 'CRP', \n",
    "                     'INSU', 'OSTE', 'VD25', 'TFS', 'ALBUMA','A1GLOA', 'A2GLOA', 'DPD']\n",
    "    tests = pd.concat([test.PATIENT_HASH, pd.DataFrame(data = enc.fit_transform(test.drop('PATIENT_HASH',axis=1)).toarray(),\n",
    "                columns = enc.get_feature_names(relevantTests))], axis = 1)\n",
    "\n",
    "    # co-morbidity\n",
    "    def ICD(row): \n",
    "        if (row['TYP'] in ['*','D']) & (pd.isnull(row['ICD10'])):\n",
    "            if ('Hyperton' in row.TEXT) & ('art' in row.TEXT):\n",
    "                return 'I10.90'\n",
    "            if 'Hepatitis C' in row.TEXT:\n",
    "                return 'B18.2'\n",
    "            if 'Hypothyreose nach medizinischen Maßnahmen' in row.TEXT:\n",
    "                return 'E89.0'\n",
    "            if 'Sterilität beim Mann' in row.TEXT:\n",
    "                return 'N46'\n",
    "            if '3-Gefäß-KHK' in row.TEXT:\n",
    "                return 'I25.13'\n",
    "            if ('Vit' in row.TEXT) & ('D' in row.TEXT) & ('Mangel' in row.TEXT):\n",
    "                return 'E55.9'\n",
    "            if 'Anämie' in row.TEXT:\n",
    "                return 'D64.9'\n",
    "            if ('fatigue' in row.TEXT) & ('yndrom' in row.TEXT):\n",
    "                return 'G93.3'\n",
    "            if 'Obstruktive Bronchitis' in row.TEXT:\n",
    "                return 'J44.89'\n",
    "            else:\n",
    "                return row['ICD10']\n",
    "        else:\n",
    "            return row['ICD10']\n",
    "    subset['ICD10'] = subset.apply(ICD, axis=1)\n",
    "    codes = ['E55.9','I10.90','D69.61','G93.3','R16.1']\n",
    "\n",
    "    coMorbidity = pd.DataFrame(tests.PATIENT_HASH)\n",
    "    for code in codes:\n",
    "        coMorbidity[code] = pd.merge(pd.DataFrame(tests.PATIENT_HASH),\n",
    "                                     subset[subset.ICD10 == code].groupby('PATIENT_HASH')['ICD10'].nunique().reset_index(),\n",
    "                                     how=\"left\", on =\"PATIENT_HASH\").fillna(0).drop(\"PATIENT_HASH\", axis=1)\n",
    "\n",
    "    # symptoms\n",
    "    words = [[\"Fatigue\", \"müde\",\"Fatique\",\"Erschöpfung\", \"fatigue\"], #fatigue\n",
    "                [\"Knochenschmerzen\",\"Knochenstoffwechselstörung\"], # bone pain\n",
    "                [\"Splenomegalie\",\"Milzläsionen\", \"splenomegalie\", \"Splenektomie\", \"Hepatosplenomegalie\"], # spenomagalie\n",
    "                [\"Thrombopenie\",\"Thrombozytopenie\"], [\"Chololithiasis\"],[\"Chitotriosidase\"],\n",
    "                [\"Anämie\"],[\"Leukopenie\"],[\"Panzytopenie\"],[\"Niereninsuffizienz\"],[\"Nephrolithiasis\"]]\n",
    "\n",
    "    symptoms = pd.DataFrame(data = subset.PATIENT_HASH.unique(), columns = ['PATIENT_HASH'])\n",
    "\n",
    "    for word in words:\n",
    "        symptoms[word[0]] = pd.merge(pd.DataFrame(data = subset.PATIENT_HASH.unique(), columns = ['PATIENT_HASH']),\n",
    "                                     subset[subset.TEXT.apply(lambda x: any(t in x for t in word))].groupby('PATIENT_HASH')['PATIENT_ID'].count().reset_index(),\n",
    "                                     how=\"left\", on =\"PATIENT_HASH\").fillna(0).drop(\"PATIENT_HASH\", axis=1)\n",
    "\n",
    "    # age\n",
    "    age = subset.groupby('PATIENT_HASH')['age'].mean().reset_index()\n",
    "\n",
    "    dataset = subset[['PATIENT_HASH','PAT_GESCHLECHT']].drop_duplicates().replace(\"W\",1).replace(\"M\",0).reset_index(drop=True)\n",
    "    dataset['gaucher'] = dataset.PATIENT_HASH.isin(patients)*1\n",
    "    dataset = pd.merge(dataset, age , how=\"left\", on='PATIENT_HASH')\n",
    "    dataset = pd.merge(dataset, tests , how=\"left\", on='PATIENT_HASH')\n",
    "    dataset = pd.merge(dataset, coMorbidity , how=\"left\", on='PATIENT_HASH')\n",
    "    dataset = pd.merge(dataset, symptoms , how=\"left\", on='PATIENT_HASH')\n",
    "    dataset = dataset.fillna(0).drop('PATIENT_HASH', axis=1)\n",
    "    \n",
    "    X = dataset.drop('gaucher', axis=1)\n",
    "    y = dataset.gaucher\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"Logistic Regression\", \n",
    "             \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "             \"Naive Bayes\"]\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        LogisticRegression(),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        MLPClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB()]\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        scores = cross_val_score(clf, X, y, cv=10)\n",
    "        y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "        CM = confusion_matrix(y, y_pred)\n",
    "        TN = CM[0][0] \n",
    "        FN = CM[1][0]\n",
    "        TP = CM[1][1]\n",
    "        FP = CM[0][1]\n",
    "        print(name,np.round(np.mean(scores),3))\n",
    "        print(\"      Positives: \", 100*round(FN/(TP+FN),2), \"% misclassifed     \", FN, '/',TP+FN)\n",
    "        print(\"      Negatives: \", 100*round(FP/(TN+FP),2), \"% misclassifed     \",FP, '/',TN+FP)\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 32 non Gaucher patients\n",
    "modelling(data, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 64 non Gaucher patients\n",
    "modelling(data, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 96 non Gaucher patients\n",
    "modelling(data, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 128 non Gaucher patients\n",
    "modelling(data, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 160 non Gaucher patients\n",
    "modelling(data, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Berlin = data[data.ZENTRUM_ID == 'BER01']\n",
    "Frankfurt = data[data.ZENTRUM_ID == 'FRA01']\n",
    "Hamburg = data[data.ZENTRUM_ID == 'HAM08']\n",
    "Stuttgart = data[data.ZENTRUM_ID == 'STR01']\n",
    "\n",
    "for center, name in zip([Berlin, Frankfurt, Hamburg, Stuttgart],['Berlin', 'Frankfurt', 'Hamburg', 'Stuttgart']):\n",
    "    print(name,\": \",\n",
    "          100 * round(center[(center.SICHERHEIT == 'G') & (center.ICD10 == 'E75.22')].PATIENT_HASH.nunique()/center.PATIENT_HASH.nunique(),5),\n",
    "         '% Gaucher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32/0.00017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 1880 non Gaucher patients\n",
    "X,y = modelling(data, 1880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation=KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "ada=AdaBoostClassifier()\n",
    "search_grid={'n_estimators':[200,500,1000,2000],'learning_rate':[.001,.01,.1,.2]}\n",
    "search=GridSearchCV(estimator=ada,param_grid=search_grid,scoring='recall_weighted',n_jobs=1,cv=crossvalidation)\n",
    "search.fit(X,y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall_score: measures the ability of a classifier to find all the positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=200, learning_rate=0.1)\n",
    "name = \"AdaBoost\"\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "CM = confusion_matrix(y, y_pred)\n",
    "TN = CM[0][0] \n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "print(name,np.round(np.mean(scores),3))\n",
    "print(\"      Positives: \", 100*round(FN/(TP+FN),2), \"% misclassifed     \", FN, '/',TP+FN)\n",
    "print(\"      Negatives: \", 100*round(FP/(TN+FP),2), \"% misclassifed     \",FP, '/',TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalances dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = AdaBoostClassifier()\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "print(\"baseline\", np.mean(cross_val_score(model, X, y, scoring='recall', cv=cv, n_jobs=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "y_pred = cross_val_predict(model, X, y,  cv=10, n_jobs=-1)\n",
    "CM = confusion_matrix(y, y_pred)\n",
    "TN = CM[0][0] \n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "print(\"      Positives: \", 100*round(FN/(TP+FN),2), \"% misclassifed     \", FN, '/',TP+FN)\n",
    "print(\"      Negatives: \", 100*round(FP/(TN+FP),2), \"% misclassifed     \",FP, '/',TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resampling\n",
    "resample = SMOTEENN()\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('r', resample), ('m', model)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Score: %.3f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X, y,  cv=10, n_jobs=-1)\n",
    "CM = confusion_matrix(y, y_pred)\n",
    "TN = CM[0][0] \n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "print(name,np.round(np.mean(scores),3))\n",
    "print(\"      Positives: \", 100*round(FN/(TP+FN),2), \"% misclassifed     \", FN, '/',TP+FN)\n",
    "print(\"      Negatives: \", 100*round(FP/(TN+FP),2), \"% misclassifed     \",FP, '/',TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation=KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "ada=AdaBoostClassifier()\n",
    "search_grid={'m__n_estimators':[200,500,1000,2000],\n",
    "             'm__learning_rate':[.001,.01,.1,.2]}\n",
    "search=GridSearchCV(pipeline,param_grid=search_grid,scoring='recall',n_jobs=1,cv=crossvalidation)\n",
    "search.fit(X,y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(learning_rate=0.001,n_estimators=200)\n",
    "pipeline = Pipeline(steps=[('r', resample), ('m', model)])\n",
    "y_pred = cross_val_predict(pipeline, X, y,  cv=10, n_jobs=-1)\n",
    "CM = confusion_matrix(y, y_pred)\n",
    "TN = CM[0][0] \n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "print(name,np.round(np.mean(scores),3))\n",
    "print(\"      Positives: \", 100*round(FN/(TP+FN),2), \"% misclassifed     \", FN, '/',TP+FN)\n",
    "print(\"      Negatives: \", 100*round(FP/(TN+FP),2), \"% misclassifed     \",FP, '/',TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
